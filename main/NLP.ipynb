{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kJJyDFNvD1am"
      },
      "source": [
        "PART I: Extracting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VZOrV3dX2L2O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "import glob\n",
        "import csv\n",
        "import copy\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7wBk_0GbBWzG"
      },
      "source": [
        "Load data from fsm_data folder. Currently, the manual analysis do have the game DisYouCatchTheBall, TheDice, and Tangram so we remove them\n",
        "\n",
        "> Note: the dir variable may differ in the local folder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDTNsRON2Q3b",
        "outputId": "fc019db4-3aa1-4211-f852-30086f4fa50a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "dir = \"../fsm-data/*.json\"\n",
        "old_files = sorted(glob.glob(dir, recursive=False))\n",
        "files = []\n",
        "for file in old_files:\n",
        "  if(file != \"../fsm-data\\TheDice_US.json\" and file != \"../fsm-data\\DidYouCatchTheBall_US.json\" and file != \"../fsm-data\\TangramsRace.json\"):\n",
        "    files.append(file)\n",
        "print(len(files))\n",
        "for single_file in files:\n",
        "    with open(single_file, 'r') as f:\n",
        "        json_file = json.load(f)\n",
        "        new_string = json.dumps(json_file, indent = 2)\n",
        "        # print(new_string)\n",
        "        data.append(json_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SGvuAmbzBkTE"
      },
      "source": [
        "Extract all of the text in the game. The considered text are from 'displayText' attribute in the json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2DHShSTq2S-Y"
      },
      "outputs": [],
      "source": [
        "c = []\n",
        "for single_file in data:\n",
        "    temp = single_file['states']\n",
        "    d = []\n",
        "    #print('------------------------------')\n",
        "    for i in temp:\n",
        "        att = list(i.keys())\n",
        "        for key in att:\n",
        "            if (key == 'displayText'):\n",
        "                s = ''\n",
        "                text = list(i[key].keys())\n",
        "                for j in text:\n",
        "                    s += i[key][j]\n",
        "                if (not (s == '')):\n",
        "                    d.append(s)\n",
        "    c.append(d)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm6B-wrTBpwc"
      },
      "source": [
        "Extracting the data from csv file in manual_data folder, and turn them into pandas DataFrame\n",
        "\n",
        "\n",
        "> Note: All csv need to be in the same name as the json file to keep the proper order (the right input goes with the right output). Furthermore, the number of files in both input and output must be the same (cannot have more input or have more output than the other)\n",
        "\n",
        "We also drop some of the unneccessary columns (criterias that seems unrelated to NLP)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MZrcmrK92VOW"
      },
      "outputs": [],
      "source": [
        "dir_csv = '../manual-analysis/*.csv'\n",
        "csv_files = sorted(glob.glob(dir_csv, recursive=False))\n",
        "for file in csv_files:\n",
        "   if(file == \"../manual-analysis\\Mortal_Gorilla_Sheet1.csv\"):\n",
        "      csv_files.remove(file)\n",
        "df_list = (pd.read_csv(file) for file in csv_files)\n",
        "labels = list(df_list)\n",
        "for y in labels:\n",
        "   y.drop(columns=[\"NAME\",\"Day\",\"Targeted Grade Level\",\"Presence of Teams\", \"Team Dynamics No Teams\", \"Team Size\", \"Number of Teams\", \"Team Dynamics Between Teams\", \"Team Dynamics Within Teams\", \"Drawing Components [Rules]\", \"Drawing Components [Physical Objects]\", \"Drawing Components [Physical Space]\", \"Drawing Components [Timing]\", \"Drawing Components [Physicality]\", \"FSMD Components [Rules]\", \"FSMD Components [Physical Objects]\", \"FSMD Components [Physical Space]\", \"FSMD Components [Timing]\", \"FSMD Components [Physicality]\", \"Presence of Finite State Machine Diagram\", \"Output State Representation\", \"Transition State Representation\", \"Finite State Machine Diagram Consistency with Specified Rules\", \"State Consistency (Boxes)\", \"Transition Consistency (Arrows)\", \"Finite State Machine Diagram Completion\", \"States/Boxes\", \"Transitions/Arrows\", \"Numbered States\", \"Evidence of Programming Language Knowledge [Arrow(s) that loop to a previous state]\"], inplace = True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nOirWrtqCiGy"
      },
      "source": [
        "Keep the input and output as a single DataFrame (may be deleted later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZuA-i1K2aeZ",
        "outputId": "3df67240-2232-4b5e-e5b0-7317c6a62b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Game Descriptor', 'Content [Counting and Cardinality ]', 'Content [Operations and Algebraic Thinking ]', 'Content [Number and Operations in Base Ten]', 'Content [Number and Operations with Fractions]', 'Content [Measurement and Data]', 'Content [Geometry]', 'Content [Ratio and Proportions]', 'Content [The Number System]', 'Content [Expressions and Equations]', 'Content [Functions]', 'Content [Statistics and Probability ]', 'Progressive Levels', 'Content Adaptability', 'Game Facilitator', 'End-Goal', 'Technological Incorporation', 'Technological Dependency', 'Player Competition (No Teams)', 'Player Collaboration', 'Team Competition', 'Team Collaboration', 'Facilitator Competition', 'Facilitator Collaboration', 'Physicality', 'Physicality Option', 'Sweat Factor', 'Physical Contact', 'Style of Physical Contact', 'Physical Space Diagram', 'Physical Environment', 'If you selected 0 (Unspecified), select one of the following codes related to the size of the environment for gameplay based on the students’ descriptions.', 'Motor Action and Math Relationship', 'Mathematical Importance', 'Mathematical Utilization', 'Written Narrative Components [Rules]', 'Written Narrative Components [Physical Objects]', 'Written Narrative Components [Physical Space]', 'Written Narrative Components [Timing]', 'Written Narrative Components [Physicality]', 'Input Types [RFID]', 'Input Types [Buttons]', 'Input Types [GPS]', 'Input Types [Keyboard]', 'Input Types [Touch Interface]', 'Input Types [Timer]', 'Input Types [Other User]', 'Specification of Mistakes', 'Domain Level: Management-Level', 'Domain Level: Team-Level', 'Domain Level: Player-Level', 'Labeled Arrows (includes input to transition; excludes numbers)', 'Evidence of Programming Language Knowledge [A reference to “If-then” Statements]', 'Evidence of Programming Language Knowledge [A reference to “While” Loops; or “Do this... until ...”]']\n"
          ]
        }
      ],
      "source": [
        "li = []\n",
        "for names in csv_files:\n",
        "  dft = pd.read_csv(names, index_col=None, header=0)\n",
        "  dft.drop(columns=[\"NAME\",\"Day\",\"Targeted Grade Level\",\"Presence of Teams\", \"Team Dynamics No Teams\", \"Team Size\", \"Number of Teams\", \"Team Dynamics Between Teams\", \"Team Dynamics Within Teams\", \"Drawing Components [Rules]\", \"Drawing Components [Physical Objects]\", \"Drawing Components [Physical Space]\", \"Drawing Components [Timing]\", \"Drawing Components [Physicality]\", \"FSMD Components [Rules]\", \"FSMD Components [Physical Objects]\", \"FSMD Components [Physical Space]\", \"FSMD Components [Timing]\", \"FSMD Components [Physicality]\", \"Presence of Finite State Machine Diagram\", \"Output State Representation\", \"Transition State Representation\", \"Finite State Machine Diagram Consistency with Specified Rules\", \"State Consistency (Boxes)\", \"Transition Consistency (Arrows)\", \"Finite State Machine Diagram Completion\", \"States/Boxes\", \"Transitions/Arrows\", \"Numbered States\", \"Evidence of Programming Language Knowledge [Arrow(s) that loop to a previous state]\"], inplace = True)\n",
        "  li.append(dft)\n",
        "frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "x = []\n",
        "for col in frame.columns:\n",
        "  if (col != 'Input'):\n",
        "    x.append(col)\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ibyey1U2caf",
        "outputId": "badcea44-92eb-4571-dcb9-640ee95840f9"
      },
      "outputs": [],
      "source": [
        "# pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LWhZ2f7e2oIF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vtminh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertModel"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MEeZ-hcqFmuH"
      },
      "source": [
        "Initializing the size for our input to feed to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxRLEjhs2yLM",
        "outputId": "16769ede-f846-4b7f-a5ab-abb58f544bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "MAX_LEN = 133\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2 \n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-05\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0YehWZ54F6JG"
      },
      "source": [
        "The following two cells create a single DataFrame that holds the input text and the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6foN3nt-3suy"
      },
      "outputs": [],
      "source": [
        "input = []\n",
        "for arr in c:\n",
        "  temp = ''\n",
        "  for i in arr:\n",
        "    i+='.'\n",
        "    temp += i\n",
        "  input.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BkzzBcD53Dje"
      },
      "outputs": [],
      "source": [
        "tempf = []\n",
        "for u in labels:\n",
        "  tempt = []\n",
        "  for k in x:\n",
        "    t = (u[k].values)[0]\n",
        "    tempt.append(t)\n",
        "  tempf.append(tempt)\n",
        "\n",
        "dataf = {'Input': input, 'Output': tempf}\n",
        "train_data = pd.DataFrame(dataf)\n",
        "# print(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "usp89VG619NB"
      },
      "source": [
        "PART II: Building the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xw_PSc65Koiz"
      },
      "source": [
        "We define a PyTorch Dataset class called MultiLabelDataset that is used to preprocess text data for multi-label text classification tasks using the DistilBERT model.\n",
        "\n",
        "We put our DataFrame into the class and it will tokenize the text (in a way needed for BERT), generate the attention mask and put all of them in a Tensor object. \n",
        "\n",
        "More info about the tokenization process: BERT needs the input to be breaks down into smaller tokens and padd all inputs to be the same length (also, all sentences must be padded with the '[CLS]' and '[SEP]' tokens at start and end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4KR8uPIL4DAY"
      },
      "outputs": [],
      "source": [
        "class MultiLabelDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len, new_data=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.Input\n",
        "        self.new_data = new_data\n",
        "        \n",
        "        if not new_data:\n",
        "            self.targets = self.data.Output\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        out = {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "        }\n",
        "        \n",
        "        if not self.new_data:\n",
        "            out['targets'] = torch.tensor(self.targets[index], dtype=torch.float)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O4IqsabILxiS"
      },
      "source": [
        "Split out data into training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXbdmNoo4ETq",
        "outputId": "81428c06-7b97-4348-b464-1224e2798363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orig Dataset: (6, 2)\n",
            "Training Dataset: (4, 2)\n",
            "Validation Dataset: (2, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.81MB/s]\n",
            "c:\\Users\\vtminh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vtminh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 3.73kB/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 60.6kB/s]\n"
          ]
        }
      ],
      "source": [
        "train_size = 0.7\n",
        "\n",
        "train_df = train_data.sample(frac=train_size, random_state=123)\n",
        "val_df = train_data.drop(train_df.index).reset_index(drop=True)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"Orig Dataset: {}\".format(train_data.shape))\n",
        "print(\"Training Dataset: {}\".format(train_df.shape))\n",
        "print(\"Validation Dataset: {}\".format(val_df.shape))\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
        "training_set = MultiLabelDataset(train_df, tokenizer, MAX_LEN)\n",
        "val_set = MultiLabelDataset(val_df, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwAVAcga4x6G",
        "outputId": "93781841-b3ab-4b85-b905-5236622ed00f"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 8\n",
        "                }\n",
        "\n",
        "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "               'shuffle': False,\n",
        "               'num_workers': 8\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KB1mOkfMJCSu"
      },
      "source": [
        "This is a model that is built up on DistilBertModel, which is a lighter, faster version of the normal Bert Model.\n",
        "\n",
        "When calling forward(), the model breaks down the inputs into hidden states, which refer to the internal representation of a sequence of text, such as a sentence or a document, that is learned by a machine learning model\n",
        "\n",
        "In the case of the DistilBERT model, the hidden state refers to the internal representation of the input text at each layer of the model. Each layer of the model takes the output of the previous layer and produces a new hidden state that captures increasingly complex and abstract features of the input text. The final hidden state of the last layer, corresponding to the [CLS] token in the input sequence, is typically used as the input to downstream tasks such as text classification, question answering, or text generation.\n",
        "\n",
        "After we have the hidden states, the model run the classifer, which is a sequence of three layers. The last layer would be a fully connected layer with 768 input neurons and 54 output neurons (54 represents our classes). The output of this classifier will be used to predict the classification label of the input text.\n",
        "\n",
        "Having multiple layers of transformers allows the model to capture increasingly complex and abstract features of the input text, as each layer can build on the representations learned by the previous layer. However, increasing the number of layers can also make the model more prone to overfitting, as it may start to memorize the training data instead of learning general patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4l-6Mrj44Kq",
        "outputId": "8f1adddd-c44c-4712-ab41-5269156d8592"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading pytorch_model.bin: 100%|██████████| 268M/268M [03:38<00:00, 1.23MB/s] \n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DistilBERTClass(\n",
              "  (bert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.1, inplace=False)\n",
              "    (3): Linear(in_features=768, out_features=54, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class DistilBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        \n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(768, 768),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.1),\n",
        "            torch.nn.Linear(768, 54)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        out = hidden_state[:, 0]\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "model = DistilBERTClass()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IzntD6D1NML6"
      },
      "source": [
        "The optimizer is an algorithm used during the training of a neural network to adjust the model's weights and biases based on the computed gradients of the loss function. The goal of the optimizer is to minimize the loss function and improve the accuracy of the model's predictions.\n",
        "The Adam optimizer is a popular optimization algorithm that is commonly used in deep learning. It is an extension of stochastic gradient descent (SGD) and is known for its ability to converge quickly and efficiently, even for large and complex models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DgzonD0K4_3O"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C5wOr4rP1BxI"
      },
      "source": [
        "During one epoch, the model receives the entire training dataset, processes it forward and backward through the network, and updates the model parameters. \n",
        "\n",
        "The train function is called for each epoch and sets the model to training mode using model.train(). It then iterates over the batches in the training_loader, loads the batch data onto the DEVICE, and passes it through the model to obtain the outputs.\n",
        "\n",
        "The optimizer's gradients are set to zero with optimizer.zero_grad() to prevent accumulation of gradients from previous batches. The loss is computed using the binary_cross_entropy_with_logits function from torch.nn.functional. This function computes the binary cross-entropy loss between the outputs and the targets.\n",
        "\n",
        "loss.backward() computes the gradients of the binary cross-entropy loss with respect to each parameter of the model, which is then used by the optimizer to update the model parameters in the next step of the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw2pTVNR5ODj",
        "outputId": "6e1886b0-c40e-4ade-96e3-6e6e0e802287"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _, data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(DEVICE, dtype=torch.long)\n",
        "        mask = data['mask'].to(DEVICE, dtype=torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(DEVICE, dtype=torch.long)\n",
        "        targets = data['targets'].to(DEVICE, dtype=torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        print(outputs)\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, targets)\n",
        "        \n",
        "        if _ % 5000 == 0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
